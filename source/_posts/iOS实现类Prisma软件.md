---
title: iOSå®ç°ç±»Prismaè½¯ä»¶
date: 2017-04-28 10:42:21
tags: [ç§»åŠ¨ç«¯, æœºå™¨å­¦ä¹ , tensorflow]
toc: true
categories:
  - å·¥ä½œ
  - iOS
thumbnail: http://upload-images.jianshu.io/upload_images/2641798-ab4749c205d9f6f8.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240
---

## å‰è¨€

---

Prisma åœ¨ 2016 ä¸Šçº¿åå°±å¤§ç«ï¼Œè¯¥ APP æ˜¯åˆ©ç”¨[ç¥ç»ç½‘ç»œ](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)å’Œ[äººå·¥æ™ºèƒ½](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD)æŠ€æœ¯ï¼Œä¸ºæ™®é€šç…§ç‰‡åŠ å…¥è‰ºæœ¯æ•ˆæœçš„ç…§ç‰‡ç¼–è¾‘è½¯ä»¶ã€‚

åŒå¹´ Google ä¹Ÿå‘å¸ƒäº†ä¸€ç¯‡ã€ŠA LEARNED REPRESENTATION FOR ARTISTIC STYLEã€‹è®ºæ–‡ï¼Œå®ç°äº†å‰å‘è¿ç®—ä¸€æ¬¡ä¸ºç…§ç‰‡æ•´åˆå¤šç§è‰ºæœ¯é£æ ¼çš„åŠŸèƒ½ï¼Œå¹¶ä¸”ä¼˜åŒ–äº†å†…å­˜ä½¿ç”¨å’Œè¿ç®—é€Ÿåº¦ï¼Œå¯ä»¥åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå¿«é€Ÿè¿ç®—ã€‚

æœ€è¿‘åœ¨ç ”ç©¶ Tensorflow æ•´åˆ iOS è¿‡ç¨‹ä¸­ï¼Œå‘ç° google å…¬å¼€äº†è®ºæ–‡å®ç°çš„æºç å’Œè®­ç»ƒæ•°æ®ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬å¯ä»¥é€šè¿‡è‡ªå·±å†™ä¸€ä¸ªå‰å‘è¿ç®—å›¾ï¼Œæ•´åˆå…¶è®­ç»ƒå‚æ•°å°±å¯ä»¥å¿«é€Ÿå®ç°ç±» Prisma çš„åº”ç”¨ã€‚

ä¸‹é¢å°±ä»‹ç»ä¸€ä¸‹å¦‚ä½•åœ¨ iPhone ä¸Šè·‘ä¸€ä¸ªè‡ªå·±çš„**"Prisma"**ã€‚

<!-- more -->

![æ‹›è´¢å’Œå’•å™œ](http://upload-images.jianshu.io/upload_images/2641798-ab4749c205d9f6f8.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## å‡†å¤‡å·¥ä½œ

---

1. å®‰è£…[Tensorflow](https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#download-and-setup)ï¼Œè¿™ä¸ªå®˜ç½‘ä¸Šæœ‰è¯¦ç»†æ•™ç¨‹è¿™é‡Œå°±ä¸å¤šè¯´äº†ã€‚
2. æ­å»º[iOS+Tensorflow](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples/)å·¥ç¨‹ï¼Œè¿™ä¸ªå¯ä»¥æ ¹æ® Git ä¸Šçš„æ­¥éª¤å®ç°ï¼Œä¹Ÿå¯ä»¥å‚è€ƒå®˜æ–¹çš„ Demo ç¨‹åºé…ç½®ã€‚ï¼ˆè¿™ä¸ªè¿‡ç¨‹æœ‰å¾ˆå¤šå‘ï¼Œå¤šæ¬¡å°è¯•ï¼Œåº”è¯¥å¯ä»¥é…ç½®æˆåŠŸï¼‰
3. ä¸‹è½½æ¨¡å‹ï¼Œæœ¬æ¬¡ä½¿ç”¨çš„æ¨¡å‹æ˜¯[image_stylization](https://github.com/tensorflow/magenta/tree/master/magenta/models/image_stylization)ï¼Œgoogle å·²å¼€æºåœ¨ GitHub ä¸Šã€‚
4. ä¸‹è½½è®­ç»ƒå¥½çš„å‚æ•°ï¼ŒGoogle æä¾›äº† 2 ä¸ªï¼š
   [Monet](http://download.magenta.tensorflow.org/models/multistyle-pastiche-generator-monet.ckpt)
   [Varied](http://download.magenta.tensorflow.org/models/multistyle-pastiche-generator-varied.ckpt)
   Monet è®­ç»ƒäº† 10 ç§è‰ºæœ¯å›¾ç‰‡ï¼ŒVaried è®­ç»ƒäº† 32 ç§ã€‚
   å½“ç„¶ä½ ä¹Ÿå¯ä»¥è‡ªå·±è®­ç»ƒè‰ºæœ¯å›¾ç‰‡ï¼Œä½†æ˜¯å¾—ä¸‹è½½ VGG çš„è®­ç»ƒå‚æ•°å’Œ ImageNet æ•°æ®ï¼Œç„¶åè‡ªå·±è®­ç»ƒï¼Œæ¯”è¾ƒèŠ±æ—¶é—´ã€‚

## æ„å»ºè®¡ç®—å›¾

---

è™½ç„¶ Google æä¾›äº†æ¨¡å‹çš„æºç ï¼Œä½†æ˜¯å¹¶æ²¡æœ‰åœ¨æºç ä¸­è¾“å‡ºè¿ç®—å›¾å·²æ–¹ä¾¿è¿ç§»åˆ°ç§»åŠ¨è®¾å¤‡ä¸­ä½¿ç”¨ï¼ŒAndroid çš„ Demo ä¸­å€’æ˜¯æä¾›äº†ç”Ÿæˆçš„ pbï¼Œå¦‚ä½•è§‰å¾—è‡ªå·±å†™è®¡ç®—å›¾éº»çƒ¦å¯ä»¥ç›´æ¥æ‹·åˆ°è‡ªå·± iOS å·¥ç¨‹ä¸­ä½¿ç”¨ã€‚

æˆ‘è¿™é‡Œåˆ›å»ºäº†ä¸€ä¸ª python çš„å·¥ç¨‹ï¼Œç„¶åæŠŠ Google æºç ä¸­ model.py ç›¸å…³çš„æ–‡ä»¶éƒ½åŠ å…¥äº†å·¥ç¨‹ã€‚
æˆ‘çš„å»ºå›¾ä»£ç å¦‚ä¸‹ï¼š

```
import numpy as np
import tensorflow as tf
import ast
import os
from tensorflow.python import pywrap_tensorflow

from matplotlib import pyplot
from matplotlib.pyplot import imshow

import image_utils
import model
import ops
import argparse
import sys


num_styles = 32
imgWidth = 512
imgHeight = 512
channel = 3
checkpoint = "/Users/Jiao/Desktop/TFProject/style-image/checkpoint/multistyle-pastiche-generator-varied.ckpt"

inputImage = tf.placeholder(tf.float32,shape=[None,imgWidth,imgHeight,channel],name="input")
styles = tf.placeholder(tf.float32,shape=[num_styles],name="style")

with tf.name_scope(""):
    transform = model.transform(inputImage,
                            normalizer_fn=ops.weighted_instance_norm,
                            normalizer_params={
                                # 'weights': tf.constant(mixture),
                                'weights' : styles,
                                'num_categories': num_styles,
                                'center': True,
                                'scale': True})

model_saver = tf.train.Saver(tf.global_variables())

with tf.Session() as sess:
    tf.train.write_graph(sess.graph_def, "/Users/Jiao/Desktop/TFProject/style-image/protobuf", "input.pb")
    #checkpoint = os.path.expanduser(checkpoint)
    #if tf.gfile.IsDirectory(checkpoint):
    #    checkpoint = tf.train.latest_checkpoint(checkpoint)
    #    tf.logging.info('loading latest checkpoint file: {}'.format(checkpoint))
    #model_saver.restore(sess, checkpoint)

    #newstyle = np.zeros([num_styles], dtype=np.float32)
    #newstyle[18] = 0.5
    #newstyle[17] = 0.5
    #newImage = np.zeros((1,imgWidth,imgHeight,channel))
    #style_image = transform.eval(feed_dict={inputImage:newImage,styles:newstyle})
    #style_image = style_image[0]
    #imshow(style_image)
    #pyplot.show()
```

è¿™é‡Œè¾“å…¥èŠ‚ç‚¹æ˜¯`input`å’Œ`style`ï¼Œè¾“å‡ºèŠ‚ç‚¹æ˜¯ model ä¸­çš„`transformer/expand/conv3/conv/Sigmoid`ã€‚

åˆ°æ­¤å°±å°†æ¨¡å‹çš„è®¡ç®—å›¾ä¿å­˜åˆ°äº†æœ¬åœ°æ–‡ä»¶å¤¹ä¸­ã€‚
æ¥ä¸‹æ¥å°±æ˜¯å°†å›¾å’Œ ckpt ä¸­çš„å‚æ•°åˆå¹¶ï¼Œå¹¶ä¸”ç”Ÿæˆç§»åŠ¨ç«¯çš„å¯ä»¥ä½¿ç”¨çš„ pb æ–‡ä»¶ï¼Œè¿™ä¸€æ­¥å¯ä»¥å‚è€ƒæˆ‘ä¸Šä¸€ç¯‡æ–‡ç« [ã€ŠiOS+Tensorflow å®ç°å›¾åƒè¯†åˆ«ã€‹](http://www.jianshu.com/p/f096fe2212ce)ï¼Œå¾ˆå®¹æ˜“å°±å®ç°ã€‚

## iOS å·¥ç¨‹

---

åœ¨ä¸Šé¢å‡†å¤‡å·¥ä½œä¸­ï¼Œå¦‚æœä½ å·²ç»æŒ‰æ­¥éª¤æ­å»ºå¥½ iOS+TF çš„å·¥ç¨‹ï¼Œè¿™é‡Œä½ åªéœ€è¦å¯¼å…¥ç”Ÿæˆçš„æœ€ç»ˆ pb æ–‡ä»¶å°±è¡Œäº†ã€‚å·¥ç¨‹ç»“æ„å¦‚å›¾ï¼š

![XCodeå·¥ç¨‹](http://upload-images.jianshu.io/upload_images/2641798-ca34abd1c875504c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

ç„¶ååœ¨ iOS ä½¿ç”¨ pb æ–‡ä»¶ï¼Œæˆ‘è¿™é‡Œç›´æ¥å¯¼å…¥äº† Google æä¾›çš„`tensorflow_utils`ï¼Œä½¿ç”¨è¿™ä¸ªç±»é‡Œé¢çš„ LoadModel æ–¹æ³•å¯ä»¥å¾ˆå¿«çš„ç”Ÿæˆå«æœ‰è®¡ç®—å›¾çš„ sessionã€‚

```
- (void)viewDidLoad {
    [super viewDidLoad];
    tensorflow::Status load_status;
    load_status = LoadModel(@"rounded_graph", @"pb", &tf_session);
    if (!load_status.ok()) {
        LOG(FATAL) << "Couldn't load model: " << load_status;
    }
    currentStyle = 0;
    isDone = true;
    _styleImageView.layer.borderColor = [UIColor grayColor].CGColor;
    _styleImageView.layer.borderWidth = 0.5;
    _ogImageView.layer.borderColor = [UIColor grayColor].CGColor;
    _ogImageView.layer.borderWidth = 0.5;
}
```

æœ€åå°±æ˜¯è·å–å›¾ç‰‡ï¼Œæ‰§è¡Œè¿ç®—ï¼Œç”Ÿæˆè‰ºæœ¯å›¾ç‰‡å±•ç¤ºã€‚è¿™é‡Œå›¾ç‰‡éœ€è¦è½¬æ¢æˆ bitmap ç„¶åè·å– data å€¼ï¼Œå±•ç¤ºå›¾ç‰‡ä¹Ÿæ˜¯ç›¸è¯†çš„è¿‡ç¨‹ã€‚å…·ä½“ä»£ç å¦‚ä¸‹ï¼š

```
- (void)runCnn:(UIImage *)compressedImg
{
    unsigned char *pixels = [self getImagePixel:compressedImg];
    int image_channels = 4;
    tensorflow::Tensor image_tensor(
                                    tensorflow::DT_FLOAT,
                                    tensorflow::TensorShape(
                                                            {1, wanted_input_height, wanted_input_width, wanted_input_channels}));
    auto image_tensor_mapped = image_tensor.tensor<float, 4>();
    tensorflow::uint8 *in = pixels;
    float *out = image_tensor_mapped.data();
    for (int y = 0; y < wanted_input_height; ++y) {
        float *out_row = out + (y * wanted_input_width * wanted_input_channels);
        for (int x = 0; x < wanted_input_width; ++x) {
            tensorflow::uint8 *in_pixel =
            in + (x * wanted_input_width * image_channels) + (y * image_channels);
            float *out_pixel = out_row + (x * wanted_input_channels);
            for (int c = 0; c < wanted_input_channels; ++c) {
                out_pixel[c] = in_pixel[c];
            }
        }
    }


    tensorflow::Tensor style(tensorflow::DT_FLOAT, tensorflow::TensorShape({32}));
    float *style_data = style.tensor<float, 1>().data();
    memset(style_data, 0, sizeof(float) * 32);
    style_data[currentStyle] = 1;

    if (tf_session.get()) {
        std::vector<tensorflow::Tensor> outputs;
        tensorflow::Status run_status = tf_session->Run(
                                                        {{contentNode, image_tensor},
                                                            {styleNode, style}},
                                                        {outputNode},
                                                        {},
                                                        &outputs);
        if (!run_status.ok()) {
            LOG(ERROR) << "Running model failed:" << run_status;
            isDone = true;
            free(pixels);
        } else {
            float *styledData = outputs[0].tensor<float,4>().data();
            UIImage *styledImg = [self createImage:styledData];
            dispatch_async(dispatch_get_main_queue(), ^{
                _styleImageView.image = styledImg;
                dispatch_after(dispatch_time(DISPATCH_TIME_NOW, (int64_t)(0.3 * NSEC_PER_SEC)), dispatch_get_main_queue(), ^{
                    isDone = true;
                    free(pixels);
                });
            });
        }
    }
}

- (unsigned char *)getImagePixel:(UIImage *)image
{
    int width = image.size.width;
    int height = image.size.height;
    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
    unsigned char *rawData = (unsigned char*) calloc(height * width * 4, sizeof(unsigned char));
    NSUInteger bytesPerPixel = 4;
    NSUInteger bytesPerRow = bytesPerPixel * width;
    NSUInteger bitsPerComponent = 8;
    CGContextRef context = CGBitmapContextCreate(rawData, width, height,

                                                 bitsPerComponent, bytesPerRow, colorSpace,

                                                 kCGImageAlphaPremultipliedLast | kCGBitmapByteOrder32Big);

    CGColorSpaceRelease(colorSpace);
    CGContextDrawImage(context, CGRectMake(0, 0, width, height), image.CGImage);
    UIImage *ogImg = [UIImage imageWithCGImage:CGBitmapContextCreateImage(context)];
    dispatch_async(dispatch_get_main_queue(), ^{
        _ogImageView.image = ogImg;
    });
    CGContextRelease(context);
    return rawData;
}

- (UIImage *)createImage:(float *)pixels
{
    unsigned char *rawData = (unsigned char*) calloc(wanted_input_height * wanted_input_width * 4, sizeof(unsigned char));
    for (int y = 0; y < wanted_input_height; ++y) {
        unsigned char *out_row = rawData + (y * wanted_input_width * 4);
        for (int x = 0; x < wanted_input_width; ++x) {
            float *in_pixel =
            pixels + (x * wanted_input_width * 3) + (y * 3);
            unsigned char *out_pixel = out_row + (x * 4);
            for (int c = 0; c < wanted_input_channels; ++c) {
                out_pixel[c] = in_pixel[c] * 255;
            }
            out_pixel[3] = UINT8_MAX;
        }
    }
    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
    NSUInteger bytesPerPixel = 4;
    NSUInteger bytesPerRow = bytesPerPixel * wanted_input_width;
    NSUInteger bitsPerComponent = 8;
    CGContextRef context = CGBitmapContextCreate(rawData, wanted_input_width, wanted_input_height,

                                                 bitsPerComponent, bytesPerRow, colorSpace,

                                                 kCGImageAlphaPremultipliedLast | kCGBitmapByteOrder32Big);

    CGColorSpaceRelease(colorSpace);
    UIImage *retImg = [UIImage imageWithCGImage:CGBitmapContextCreateImage(context)];
    CGContextRelease(context);
    free(rawData);
    return retImg;
}
```

_è¿™é‡Œè¯´æ˜ä¸€ä¸‹ï¼Œå‰é¢ python å·¥ç¨‹å·²ç»å®šä¹‰äº†ï¼Œæˆ‘çš„è¾“å…¥å’Œè¾“å‡ºå›¾ç‰‡çš„å¤§å°æ˜¯ 512âœ•512ã€‚_

## è¿æ¥ iPhoneï¼Œè¿è¡Œå·¥ç¨‹

---

æœ€åè¿ä¸Šæ‰‹æœºè¿è¡Œï¼Œå°±å¯ä»¥è‡ªå·±åˆ›å»ºè‡ªå·±çš„è‰ºæœ¯ç±»å›¾ç‰‡äº†ã€‚ğŸ˜Š

æ”¾å‡ å¼ è¿è¡Œæ•ˆæœå›¾ï¼š
![æˆªå›¾1](http://upload-images.jianshu.io/upload_images/2641798-d105c1089f62c634.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![æˆªå›¾2](http://upload-images.jianshu.io/upload_images/2641798-6b722e433ca66c5b.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![æˆªå›¾3](http://upload-images.jianshu.io/upload_images/2641798-1b5acd6620b5b447.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
